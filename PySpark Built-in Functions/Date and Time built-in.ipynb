{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf28a2fa-cf30-449c-a1ef-3cca9a2cf016",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c2e58b-a215-4f09-9c4a-53d6ef75f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eba238-556f-449f-b303-764e085d4467",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed46433-930c-48bf-a40b-bf9203466308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"TIME\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9058304b-66e8-4aac-82d7-cc9dd9e9c399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ffea8-02ef-48f5-bee9-0ccb04df24c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Casting to TimeStamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b03a75-531c-4969-876b-51f100cf44d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### to_timestamp() (for casting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5393f2e3-b0ba-4bbc-8ead-c6c9e13ab99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df.select(\"id\", to_timestamp(\"input_timestamp\").alias(\"timestamp\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b698e91d-dedb-4027-a71c-21737e8b7a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n",
      "+---+-----------------------+-------------------+\n",
      "|id |input_timestamp        |timestamp          |\n",
      "+---+-----------------------+-------------------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24 12:01:19|\n",
      "+---+-----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"timestamp\", to_timestamp(\"input_timestamp\"))\n",
    "df2.printSchema()\n",
    "df2.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "add315ce-8e0a-4098-aac2-891ece6d95ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df2.select(col(\"id\"), col(\"timestamp\"), col(\"timestamp\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bb4eda3-7283-4066-849a-8056fb2058d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-------------------+-------------------+\n",
      "|id |input_timestamp        |timestamp          |current            |\n",
      "+---+-----------------------+-------------------+-------------------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24 12:01:19|09-02-2024 09:59:25|\n",
      "+---+-----------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, date_format\n",
    "df2.withColumn(\"current\", date_format(current_timestamp(), \"dd-MM-yyyy HH:mm:ss\"))\\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d175a-89b5-40d2-a2eb-7ebecdc8ad6b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using SQL Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6252c6e7-33f2-44a3-804b-c5737f17fc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-------------------+\n",
      "| id|         time_stamp|        create_time|\n",
      "+---+-------------------+-------------------+\n",
      "|  1|2019-06-24 12:01:19|09/02/2024 10:09:28|\n",
      "+---+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"sample_time\")\n",
    "spark.sql(\"select int(id), to_timestamp(input_timestamp) as time_stamp, date_format(current_timestamp(), 'dd/MM/yyyy HH:mm:ss') as create_time from sample_time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce373623-f9bb-43ab-a43a-f1df445deaaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|timestamp          |\n",
      "+-------------------+\n",
      "|2019-06-24 12:01:19|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select to_timestamp('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as timestamp\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7408973-3f1e-4d61-a1e2-23a3fa0be041",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## to_date() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b73512-90fc-4d98-8a92-5fbd7260ba9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0db95a4b-b1c6-4269-9945-1b358a6d3ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2019-06-24 12:01:19.000\")],\n",
    "        schema=[\"id\",\"input_timestamp\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "636cbd8d-f45a-46d1-ad81-130723d0f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date      |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df.withColumn(\"date\", to_date(\"input_timestamp\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fbed984e-927e-499f-8135-394274981703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date      |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.withColumn(\"date\", to_date(\"input_timestamp\",'yyyy-MM-dd HH:mm:ss.SSSS')).show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88821659-33e8-4a31-b45b-ecc117e6e221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-------------------+----------+\n",
      "|id |input_timestamp        |ts                 |date      |\n",
      "+---+-----------------------+-------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24 12:01:19|2019-06-24|\n",
      "+---+-----------------------+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.withColumn(\"ts\", to_timestamp(col(\"input_timestamp\")))\\\n",
    "    .withColumn(\"date\", to_date(col(\"ts\")))\\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8f48b61-9fa6-4039-abfd-fbed9f81f54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date_type |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"date_type\", col(\"input_timestamp\").cast(\"date\"))\\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "238ccdce-ae29-4545-85f2-f92e26651d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+----------+\n",
      "|id |input_timestamp        |date      |\n",
      "+---+-----------------------+----------+\n",
      "|1  |2019-06-24 12:01:19.000|2019-06-24|\n",
      "+---+-----------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('date', to_timestamp('input_timestamp').cast('date'))\\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c168449-3857-41a3-bc67-2db2c94c5635",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "aa47f351-b26a-42d6-9240-29ecd8fa23eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| date_type|\n",
      "+----------+\n",
      "|2019-06-24|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date('2019-06-24 12:01:19.000') as date_type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16db4ce0-282c-4d1d-960b-c5644a67a1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| date_type|\n",
      "+----------+\n",
      "|2019-06-24|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select date(to_timestamp('2019-06-24 12:01:19.000')) as date_type\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1bd3508a-25b6-4904-8bd2-dba9a48018cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| date_type|\n",
      "+----------+\n",
      "|2019-06-24|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select to_date('06-24-2019 12:01:19.000','MM-dd-yyyy HH:mm:ss.SSSS') as date_type\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe07c5-20e7-4361-90e5-e60fc7ee38f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## date_format() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d585a85-dd88-4878-90ef-4fce1a7ad3cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1bd33f1e-14d8-4ca0-a99e-f7e9171be5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.createDataFrame([[\"1\"]],[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55e1dd-f442-45c4-b5e0-e8b0ad8b1941",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Various Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e9736b4-239b-471d-bbd3-2ba9ae2199e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------------+-----------+----------------+----------------------+------------+\n",
      "|current_date|yyyy-MM-dd|yyyy/MM/dd HH:mm|yyyy MMM dd|dd MMMM yyyy    |am/pm                 |HH:mm day   |\n",
      "+------------+----------+----------------+-----------+----------------+----------------------+------------+\n",
      "|2024-02-09  |2024-02-09|2024/02/09 14:30|2024 Feb 09|09 February 2024|09-02-2024 02:30:51 PM|14:30 Friday|\n",
      "+------------+----------+----------------+-----------+----------------+----------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "df.select(current_date().alias(\"current_date\"),\n",
    "         date_format(current_timestamp(), \"yyyy-MM-dd\").alias(\"yyyy-MM-dd\"),\n",
    "         date_format(current_timestamp(), \"yyyy/MM/dd HH:mm\").alias(\"yyyy/MM/dd HH:mm\"),\n",
    "         date_format(current_timestamp(), \"yyyy MMM dd\").alias(\"yyyy MMM dd\"),\n",
    "         date_format(current_timestamp(), \"dd MMMM yyyy\").alias(\"dd MMMM yyyy\"),\n",
    "         date_format(current_timestamp(), \"dd-MM-yyyy hh:mm:ss a\").alias(\"am/pm\"),\n",
    "         date_format(current_timestamp(), \"HH:mm EEEE\").alias(\"HH:mm day\"))\\\n",
    "        .show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cac2d-4567-4ce0-a376-791c59bd884d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## datediff() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d2b96a7-418b-420e-a354-0e1d93fa633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"1\",\"2019-07-01\"),(\"2\",\"2019-06-24\"),(\"3\",\"2019-08-24\")]\n",
    "df=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "484b9098-d1b7-4b4c-9960-879b8e17403a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------------+\n",
      "|      date|     today|datediff(current_date(), date)|\n",
      "+----------+----------+------------------------------+\n",
      "|2019-07-01|2024-02-09|                          1684|\n",
      "|2019-06-24|2024-02-09|                          1691|\n",
      "|2019-08-24|2024-02-09|                          1630|\n",
      "+----------+----------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "df.select(col(\"date\"),\n",
    "         current_date().alias(\"today\"),\n",
    "         datediff(current_date(), col(\"date\")))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d385af-4fba-4476-995f-f386e8c8d088",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## months_between() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c1ab0ae-eec3-41c7-9bc9-622eebb7d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+----------------------+-------------+\n",
      "| id|      date|months_between|months_between_rounded|years_between|\n",
      "+---+----------+--------------+----------------------+-------------+\n",
      "|  1|2019-07-01|   55.25806452|                 55.26|          4.6|\n",
      "|  2|2019-06-24|   55.51612903|                 55.52|         4.63|\n",
      "|  3|2019-08-24|   53.51612903|                 53.52|         4.46|\n",
      "+---+----------+--------------+----------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "from pyspark.sql.functions import months_between\n",
    "df.withColumn(\"months_between\", months_between(current_date(), col(\"date\")))\\\n",
    "    .withColumn(\"months_between_rounded\", round(months_between(current_date(), col(\"date\")), 2))\\\n",
    "    .withColumn(\"years_between\", round(months_between(current_date(), col(\"date\"))/ lit('12'), 2))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c8a52c-6295-468c-a7c9-1ce639f8bebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
