{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4075fd5e-eae8-4ef9-9f69-8d2e47fb62a3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7cc4e5b-b1fa-473c-a278-82c46e97023a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbff17-6146-455f-8134-97085f1da38e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0fbcdfd-98e3-42f6-bd50-d3bf67f30a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Otherwise-when\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a0d92-ee05-4189-8c3b-6b249d663f8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## when() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c36a1-aa5c-4d56-a0ad-fafb8eca9589",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3b0423e-c13f-4962-a0df-2075524c7d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|     M| 60000|\n",
      "|Michael|     M| 70000|\n",
      "| Robert|  null|400000|\n",
      "|  Maria|     F|500000|\n",
      "|    Jen|      |  null|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James\",\"M\",60000),(\"Michael\",\"M\",70000),\n",
    "        (\"Robert\",None,400000),(\"Maria\",\"F\",500000),\n",
    "        (\"Jen\",\"\",None)]\n",
    "\n",
    "columns = [\"name\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data = data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59b182d-845f-4803-9630-59169217766f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using With Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f96b9ff-a00d-4b0f-8b44-b2600d916516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  null|400000|       N/A|\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  null|       N/A|\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df2 = df.withColumn(\"new_gender\", when(df.gender == 'M', 'Male')\n",
    "                                   .when(df.gender == 'F', 'Female')\n",
    "                                   .otherwise('N/A'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e2314-af83-4c57-94b0-7075f9108e38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using select() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7bd08cb-c3f4-4f91-b22f-f872cfd3afed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  null|400000|       N/A|\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  null|       N/A|\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df3=df.select(col(\"*\"),when(df.gender == \"M\",\"Male\")\n",
    "                  .when(df.gender == \"F\",\"Female\")\n",
    "                  .otherwise(\"N/A\").alias(\"new_gender\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50ca19-c635-40d8-aa1f-4cde11157527",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using expr (Case, When) in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75310866-1c7f-404e-9f54-2922a5ee79bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+\n",
      "|   name|gender|salary|new_Gender|\n",
      "+-------+------+------+----------+\n",
      "|  James|     M| 60000|      Male|\n",
      "|Michael|     M| 70000|      Male|\n",
      "| Robert|  null|400000|        Na|\n",
      "|  Maria|     F|500000|    Female|\n",
      "|    Jen|      |  null|        Na|\n",
      "+-------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df4 = df.withColumn(\"new_Gender\", expr(\n",
    "    \"CASE WHEN gender = 'M' Then 'Male'\" +\n",
    "    \"WHEN gender = 'F' Then 'Female'\" + \n",
    "    \"ELSE 'Na' END\"))\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b642f53d-6f6b-488a-a0f0-aac1c66fd523",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using Case on WHEN SQL Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffc6ebd7-ae9d-41e6-be1b-516e37254f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+\n",
      "|   name|gender|salary|\n",
      "+-------+------+------+\n",
      "|  James|  MALE| 60000|\n",
      "|Michael|  MALE| 70000|\n",
      "| Robert|    NA|400000|\n",
      "|  Maria|FEMALE|500000|\n",
      "|    Jen|    NA|  null|\n",
      "+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"sample\")\n",
    "df_sql = spark.sql(\"\"\"\n",
    "            SELECT name, \n",
    "            CASE\n",
    "            WHEN gender = 'M' THEN 'MALE'\n",
    "            WHEN gender = 'F' THEN 'FEMALE'\n",
    "            ELSE 'NA'\n",
    "            END as gender,\n",
    "            salary\n",
    "            FROM sample\n",
    "        \"\"\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db6e856-5c88-48b5-b444-180f14786c1a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## expr() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6747f2-9c73-4259-8423-050ace9e48ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46b75bc4-6fcd-460f-8fa7-9dbb7f97a9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| col1| col2|\n",
      "+-----+-----+\n",
      "|James| Bond|\n",
      "|Scott|Varsa|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[(\"James\",\"Bond\"),(\"Scott\",\"Varsa\")] \n",
    "df=spark.createDataFrame(data).toDF(\"col1\",\"col2\") \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c3433ca-424f-4aec-9ab0-8d9dc9347e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|gen|\n",
      "+--------+---+\n",
      "|   James|  M|\n",
      "|Praveena|  F|\n",
      "|Yaswanth|   |\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1 = [\n",
    "    (\"James\", \"M\"),\n",
    "    (\"Praveena\", \"F\"),\n",
    "    (\"Yaswanth\", \"\")\n",
    "]\n",
    "col1 = [\"name\", \"gen\"]\n",
    "df1 = spark.createDataFrame(data1, col1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a09ea7a3-bb2b-4bdf-ba22-5b8a6aa67d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      date|increment|\n",
      "+----------+---------+\n",
      "|2019-01-23|        1|\n",
      "|2019-06-24|        2|\n",
      "|2019-09-20|        3|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)] \n",
    "df2=spark.createDataFrame(data2).toDF(\"date\",\"increment\") \n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d10b4-8573-47b2-85d7-d955155d1c0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Concatenate using expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97929e17-e714-4bdf-9aa0-61c4d257fae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----------+\n",
      "| col1| col2|       Name|\n",
      "+-----+-----+-----------+\n",
      "|James| Bond| James Bond|\n",
      "|Scott|Varsa|Scott Varsa|\n",
      "+-----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Name\", expr(\"col1 ||' '|| col2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4da442-ded3-4093-bd8c-9b8880c26403",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### When using expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7fcd1c6-c802-4bc4-a6bf-aa8f931ca0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+------+\n",
      "|    name|gen|gender|\n",
      "+--------+---+------+\n",
      "|   James|  M|  MALE|\n",
      "|Praveena|  F|FEMALE|\n",
      "|Yaswanth|   |    NA|\n",
      "+--------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn(\"gender\", expr(\n",
    "    \"\"\"\n",
    "       CASE\n",
    "        WHEN gen = 'M' THEN 'MALE'\n",
    "        WHEN gen = 'F' THEN 'FEMALE'\n",
    "        ELSE 'NA'\n",
    "        END\n",
    "    \"\"\"\n",
    ")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c151d-868e-4f8c-8f56-2d1759328ee4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using Existing Column Value for expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6068588-90f6-4fbd-99d3-9e0447782640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------------+\n",
      "|      date|increment|updated_timestamp|\n",
      "+----------+---------+-----------------+\n",
      "|2019-01-23|        1|       2019-02-23|\n",
      "|2019-06-24|        2|       2019-08-24|\n",
      "|2019-09-20|        3|       2019-12-20|\n",
      "+----------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.withColumn(\"updated_timestamp\", expr(\n",
    "    'add_months(date, increment)'\n",
    ")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c29b8ea-3d0f-4528-931f-e9c9d7a9d057",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using alias with the column expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "955e41b7-f6d9-4bdc-a7d0-f30e1124930b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+\n",
      "|      date|increment|  new_date|\n",
      "+----------+---------+----------+\n",
      "|2019-01-23|        1|2019-02-23|\n",
      "|2019-06-24|        2|2019-08-24|\n",
      "|2019-09-20|        3|2019-12-20|\n",
      "+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(df2.date, df2.increment, expr(\n",
    "    'add_months(date, increment) as new_date'\n",
    ")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743883a8-8e9b-45a4-ad9e-f612a7b557c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### cast function with expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58b3b0a2-da01-4b64-865e-7d2327149688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- increment: long (nullable = true)\n",
      " |-- increment_str: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"increment\", expr(\"cast(increment as string) as increment_str\"))\\\n",
    "    .printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165a6118-fc64-4faf-832e-1d290e679282",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0903b24-8ce5-4f60-876c-6fb858388587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|      date|increment|new_increment|\n",
      "+----------+---------+-------------+\n",
      "|2019-01-23|        1|            5|\n",
      "|2019-06-24|        2|            6|\n",
      "|2019-09-20|        3|            7|\n",
      "+----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(df2.date,df2.increment, expr(\n",
    "    \"increment + 4 as new_increment\"\n",
    ")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c52882-7860-4944-998c-053a69cad5de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Filter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca224135-5b12-4ea6-ace2-6eb656901962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "| 100|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data3=[(100,2),(200,3000),(500,500)] \n",
    "df3=spark.createDataFrame(data3).toDF(\"col1\",\"col2\")\n",
    "df3.filter(expr(\"col1 >col2\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c99954-91ce-45b1-8da7-6ba152fde5d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## lit() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cf6d9-791a-4dfa-bdf3-76ce46ac85cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f6aea56-498b-46e8-9bfb-0e14fd90ee08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|EmpId|Salary|\n",
      "+-----+------+\n",
      "|  111| 50000|\n",
      "|  222| 60000|\n",
      "|  333| 40000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"111\",50000),(\"222\",60000),(\"333\",40000)]\n",
    "df = spark.createDataFrame(data, [\"EmpId\", \"Salary\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37878f45-6035-40a3-accd-3b3b1db98d2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### lit() with select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8589293c-e96b-4cfa-bc31-ea29e52e36b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+\n",
      "|EmpId|Salary|lit|\n",
      "+-----+------+---+\n",
      "|  111| 50000|  1|\n",
      "|  222| 60000|  1|\n",
      "|  333| 40000|  1|\n",
      "+-----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(\"EmpId\", \"Salary\", lit(1).alias(\"lit\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1244154-206a-4c46-abec-46c3b6e609d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### lit with withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c14956f1-f90a-4f70-8b43-2e1a1b481b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+\n",
      "|EmpId|Salary|new_col|\n",
      "+-----+------+-------+\n",
      "|  111| 50000|    200|\n",
      "|  222| 60000|    200|\n",
      "|  333| 40000|    100|\n",
      "+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_col\", when(df.Salary > 40000, lit(200)).otherwise(lit(100))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0a7dc-e8e9-4513-848a-20a87770463e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### typedLit() withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "554e4430-4123-4dfa-b1bf-add99a9ccbef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'typedLit' from 'pyspark.sql.functions' (C:\\spark\\spark-3.4.2-bin-hadoop3\\python\\pyspark\\sql\\functions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StringType\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m typedLit\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_typed\u001b[39m\u001b[38;5;124m\"\u001b[39m, typedLit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflag\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType()))\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'typedLit' from 'pyspark.sql.functions' (C:\\spark\\spark-3.4.2-bin-hadoop3\\python\\pyspark\\sql\\functions.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import typedLit\n",
    "df.withColumn(\"new_typed\", typedLit(\"flag\", StringType())).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
