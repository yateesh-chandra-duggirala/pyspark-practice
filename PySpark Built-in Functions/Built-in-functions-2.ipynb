{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0fede11-4743-4ac8-8c70-1a24b7e242b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4486928-ed03-460f-9352-3661630ba097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a4cdc-b88b-4cac-b424-6e13eedb8132",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Creating Sparksession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f27a882-dfda-4dcc-9fd6-a29660e2ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"built-in2\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1628b6e-2c33-4cd3-bc73-b9dd041ebc07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## split() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d0089-0c54-4c90-8128-096e74bbc99e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba18d78-b6cf-4a89-879e-34b610c6c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+------+------+\n",
      "|                name|dob_year|gender|salary|\n",
      "+--------------------+--------+------+------+\n",
      "|     James, A, Smith|    2018|     M|  3000|\n",
      "|Michael, Rose, Jones|    2010|     M|  4000|\n",
      "|   Robert,K,Williams|    2010|     M|  4000|\n",
      "|    Maria,Anne,Jones|    2005|     F|  4000|\n",
      "|      Jen,Mary,Brown|    2010|      |    -1|\n",
      "+--------------------+--------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"James, A, Smith\",\"2018\",\"M\",3000),\n",
    "            (\"Michael, Rose, Jones\",\"2010\",\"M\",4000),\n",
    "            (\"Robert,K,Williams\",\"2010\",\"M\",4000),\n",
    "            (\"Maria,Anne,Jones\",\"2005\",\"F\",4000),\n",
    "            (\"Jen,Mary,Brown\",\"2010\",\"\",-1)\n",
    "            ]\n",
    "\n",
    "columns=[\"name\",\"dob_year\",\"gender\",\"salary\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c644a-a11e-4ffa-a38e-24f97bfc726c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Converting String to Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9f58c8-4679-4f8f-a9ff-dece8e36f630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|full_name               |\n",
      "+------------------------+\n",
      "|[James,  A,  Smith]     |\n",
      "|[Michael,  Rose,  Jones]|\n",
      "|[Robert, K, Williams]   |\n",
      "|[Maria, Anne, Jones]    |\n",
      "|[Jen, Mary, Brown]      |\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "# split is used to convert any string into array/ list\n",
    "df.select(split(df.name, \",\").alias(\"full_name\"))\\\n",
    "    .drop(\"name\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a6ec9-57fd-4b4f-8947-4f6bac51d6e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Converting String to Array using SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a109c8d-46b7-466a-9983-e72f705341c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                name|           full_name|\n",
      "+--------------------+--------------------+\n",
      "|     James, A, Smith| [James,  A,  Smith]|\n",
      "|Michael, Rose, Jones|[Michael,  Rose, ...|\n",
      "|   Robert,K,Williams|[Robert, K, Willi...|\n",
      "|    Maria,Anne,Jones|[Maria, Anne, Jones]|\n",
      "|      Jen,Mary,Brown|  [Jen, Mary, Brown]|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"employee_hr\")\n",
    "spark.sql(\"\"\"\n",
    "            SELECT name, SPLIT(name, \",\") as full_name FROM employee_hr\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5271b-c583-431c-ae0c-dd4a334a41f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## concat_ws() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c5859-90d2-4967-8bc4-89db27f3a828",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b70bba28-c049-47af-859d-fccbe05e9fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- languagesAtSchool: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- currentState: string (nullable = true)\n",
      "\n",
      "+----------------+------------------+------------+\n",
      "|name            |languagesAtSchool |currentState|\n",
      "+----------------+------------------+------------+\n",
      "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
      "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
      "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
      "+----------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
    "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
    "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
    "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97a0c7-1c99-423b-a2b8-6c9cbc6372af",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Combining array elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d6fa899-1c26-4b56-b3c7-7fc091526702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "| languagesAtSchool|       new_col|\n",
      "+------------------+--------------+\n",
      "|[Java, Scala, C++]|Java;Scala;C++|\n",
      "|[Spark, Java, C++]|Spark;Java;C++|\n",
      "|      [CSharp, VB]|     CSharp;VB|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "df.select(df.languagesAtSchool)\\\n",
    "    .withColumn(\"new_col\", concat_ws(\";\", df.languagesAtSchool))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b479a-eb65-46bf-a97c-60b9ee053527",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13991f18-8ce4-470c-ba24-d05efd5fba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|            name|          lang|\n",
      "+----------------+--------------+\n",
      "|    James,,Smith|Java;Scala;C++|\n",
      "|   Michael,Rose,|Spark;Java;C++|\n",
      "|Robert,,Williams|     CSharp;VB|\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"student\")\n",
    "spark.sql(\"\"\"\n",
    "            SELECT name, CONCAT_WS(\";\", languagesAtSchool) as lang\n",
    "            FROM student\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db33b7-85d8-4ff7-ae5a-b69f89985b66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## substring() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fddc0e5-24a1-4e2b-a59a-aca4dbecfe5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1880f3-bc0c-4fcb-a2db-9a0d6f58b3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|calendar|\n",
      "+---+--------+\n",
      "|  1|20200828|\n",
      "|  2|20180525|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1,\"20200828\"),(2,\"20180525\")]\n",
    "columns=[\"id\",\"calendar\"]\n",
    "df=spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753c8d3-3daf-4f75-a707-72a94eeeb5f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### withColumn using substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6ce6266-78d8-423e-a623-bfc1d142eacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-----+----+\n",
      "| id|calendar|Year|Month|Date|\n",
      "+---+--------+----+-----+----+\n",
      "|  1|20200828|2020|   08|  28|\n",
      "|  2|20180525|2018|   05|  25|\n",
      "+---+--------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "df.withColumn(\"Year\",substring(df.calendar, 1, 4))\\\n",
    "    .withColumn(\"Month\", substring(df.calendar, 5, 2))\\\n",
    "    .withColumn(\"Date\",  substring(df.calendar, 7, 2))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3c9e6-6a0f-42bb-8851-c6f148200154",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### select using substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97feea5f-1ee7-4b4d-ab5a-2200c4b9b313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+----+\n",
      "|calendar|Year|Month|Date|\n",
      "+--------+----+-----+----+\n",
      "|20200828|2020|   08|  28|\n",
      "|20180525|2018|   05|  25|\n",
      "+--------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"calendar\", substring(df.calendar, 1, 4).alias('Year'),\n",
    "         substring(df.calendar, 5, 2).alias('Month'),\n",
    "         substring(df.calendar, 7, 2).alias('Date'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17866080-23a8-4890-9b71-e360f9aa7d06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### with selectExpr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee918608-d0ef-40be-9ef9-a3cb283460fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----+----+\n",
      "|calendar|year|month|date|\n",
      "+--------+----+-----+----+\n",
      "|20200828|2020|   08|  28|\n",
      "|20180525|2018|   05|  25|\n",
      "+--------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr('calendar', 'substring(calendar, 1, 4) as year',\n",
    "             'substring(calendar, 5, 2) as month',\n",
    "             'substring(calendar, 7, 2) as date')\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b0bd4b-d285-4c4f-aabe-d943f3c4ee0e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### substr() from column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8663f4c9-d0b1-4fa6-bebb-7ba6bb7a54a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-----+----+\n",
      "| id|calendar|year|month|date|\n",
      "+---+--------+----+-----+----+\n",
      "|  1|20200828|2020|   08|  28|\n",
      "|  2|20180525|2018|   05|  25|\n",
      "+---+--------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"year\", df.calendar.substr(1, 4))\\\n",
    "    .withColumn(\"month\", df.calendar.substr(5, 2))\\\n",
    "    .withColumn(\"date\", df.calendar.substr(7, 2))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b39db-701f-41da-8399-0c006094e457",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29d6031d-4238-442c-9955-3ba042962bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-----+----+\n",
      "| id|calendar|year|month|date|\n",
      "+---+--------+----+-----+----+\n",
      "|  1|20200828|2020|   08|  28|\n",
      "|  2|20180525|2018|   05|  25|\n",
      "+---+--------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"details\")\n",
    "spark.sql(\"\"\"\n",
    "            SELECT id, calendar,\n",
    "            SUBSTRING(calendar, 1, 4) as year,\n",
    "            SUBSTRING(calendar, 5, 2) as month,\n",
    "            SUBSTRING(calendar, 7, 2) as date\n",
    "            from details\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88ae9a60-eb8b-4cb2-82e3-21b8b86797a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----+-----+----+\n",
      "| id|calendar|year|month|date|\n",
      "+---+--------+----+-----+----+\n",
      "|  1|20200828|2020|   08|  28|\n",
      "|  2|20180525|2018|   05|  25|\n",
      "+---+--------+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "select_statement = \"SELECT id, calendar, SUBSTRING(calendar, 1, 4) as year, SUBSTRING(calendar, 5, 2) as month, SUBSTRING(calendar, 7, 2) as date from details\"\n",
    "spark.sql(f\"{select_statement}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c796f1d1-a725-4547-81d9-070da998bf87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## regexp_replace() function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3a285f-2acd-482e-9946-7748b18de770",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4343ddf-ae55-4a76-b047-78f22a1fd077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+\n",
      "| id|           address|state|\n",
      "+---+------------------+-----+\n",
      "|  1|  14851 Jeffrey Rd|   DE|\n",
      "|  2|43421 Margarita St|   NY|\n",
      "|  3|  13111 Siemon Ave|   CA|\n",
      "+---+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "address = [(1,\"14851 Jeffrey Rd\",\"DE\"),\n",
    "    (2,\"43421 Margarita St\",\"NY\"),\n",
    "    (3,\"13111 Siemon Ave\",\"CA\")]\n",
    "df = spark.createDataFrame(address,[\"id\",\"address\",\"state\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc4fef-dfef-4ad3-afdf-b10e8296e8cc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Replace String with Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2652be5f-407d-4b14-aeb9-25b02b6d0fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------------------+\n",
      "| id|state|          new_addr|\n",
      "+---+-----+------------------+\n",
      "|  1|   DE|14851 Jeffrey Road|\n",
      "|  2|   NY|43421 Margarita St|\n",
      "|  3|   CA|  13111 Siemon Ave|\n",
      "+---+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "df.withColumn(\"new_addr\", regexp_replace('address', 'Rd', 'Road'))\\\n",
    "    .drop(\"address\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4f3a48-395c-4916-aa16-493f81c4d435",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Replace Column values conditionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ae348a7-d217-4648-b2e1-dcc3083ca0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------+-----+\n",
      "|id |address               |state|\n",
      "+---+----------------------+-----+\n",
      "|1  |14851 Jeffrey Road    |DE   |\n",
      "|2  |43421 Margarita Street|NY   |\n",
      "|3  |13111 Siemon Avenue   |CA   |\n",
      "+---+----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df.withColumn('address',\n",
    "             when(df.address.endswith(\"Rd\"),regexp_replace(df.address, 'Rd', 'Road'))\\\n",
    "             .when(df.address.endswith(\"St\"), regexp_replace(df.address, 'St', 'Street'))\\\n",
    "             .when(df.address.endswith(\"Ave\"), regexp_replace(df.address, 'Ave', 'Avenue'))\\\n",
    "            .otherwise(df.address))\\\n",
    "            .show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19ffb2-aad6-4000-af72-8ae1b8f7b157",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Replace column with another column value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4762e27-1c5f-4428-8d0d-51de41e33016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.createDataFrame(\n",
    "   [(\"ABCDE_XYZ\", \"XYZ\",\"FGH\")], \n",
    "    (\"col1\", \"col2\",\"col3\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eec1cb3e-7d3b-4995-8869-10946c041db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+---------+\n",
      "|     col1|col2|col3|  new_col|\n",
      "+---------+----+----+---------+\n",
      "|ABCDE_XYZ| XYZ| FGH|ABCDE_FGH|\n",
      "+---------+----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# col1 is the column that has to be replaced with wherever the values from the col2 should be replaced with col3\n",
    "df3.withColumn(\"new_col\",\n",
    "              expr(\"regexp_replace(col1, col2, col3)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc9c6c-b43e-4ddd-b525-7eb4314d3148",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Replace column values with dictionary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62de469e-cfd4-42a3-94f1-d5f71449d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+----------+\n",
      "| id|           address|     state|\n",
      "+---+------------------+----------+\n",
      "|  1|  14851 Jeffrey Rd|  Delaware|\n",
      "|  2|43421 Margarita St|   NewYork|\n",
      "|  3|  13111 Siemon Ave|California|\n",
      "+---+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stateDict = {\"DE\":\"Delaware\", \"NY\":\"NewYork\", \"CA\":\"California\"}\n",
    "df2 = df.rdd.map(lambda x :\n",
    "                (x.id, x.address, stateDict[x.state])\n",
    "                ).toDF([\"id\", \"address\", \"state\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a3a6c-d88f-4b00-b9bf-ff2a037e2aa1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## translate() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31adb46-5b6b-4a4c-a5eb-fc90c362ad1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----+------------------+\n",
      "| id|           address|state|           new_add|\n",
      "+---+------------------+-----+------------------+\n",
      "|  1|  14851 Jeffrey Rd|   DE|  A485A Jeffrey Rd|\n",
      "|  2|43421 Margarita St|   NY|4C4BA Margarita St|\n",
      "|  3|  13111 Siemon Ave|   CA|  ACAAA Siemon Ave|\n",
      "+---+------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "df.withColumn(\"new_add\", translate('address','123','ABC')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ff6c4-4998-42a1-a31c-c6c43b6d361b",
   "metadata": {},
   "source": [
    "## overlay() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bc7f3a4-c7b0-417c-b14b-e96b82e757a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: string (nullable = true)\n",
      " |-- col2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.createDataFrame([(\"ABCDE_XYZ\", \"FGH\")], (\"col1\", \"col2\"))\n",
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5bddb09-a18e-430b-a9fe-aec31c3946a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-----------+\n",
      "|     col1|col2|new_overlay|\n",
      "+---------+----+-----------+\n",
      "|ABCDE_XYZ| FGH|ABCDE_XYFGH|\n",
      "+---------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import overlay\n",
    "df4.withColumn(\"new_overlay\", overlay(\"col1\", \"col2\", 9)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
