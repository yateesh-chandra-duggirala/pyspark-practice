{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d411de8",
   "metadata": {},
   "source": [
    "# Pyspark Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec4b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b524566",
   "metadata": {},
   "source": [
    "# Get started With First Spark Session Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "788167dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create a Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Sparksession\") \\\n",
    "        .master(\"local\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7411a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "| Alice| 23|\n",
      "|   Bob| 30|\n",
      "|Mahesh| 21|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the data\n",
    "data = [(\"Alice\", 23),(\"Bob\", 30),(\"Mahesh\", 21)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "#Let us show the data using show() method\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d9860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7dec0",
   "metadata": {},
   "source": [
    "# Difference Between SparkContext and SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada99ee",
   "metadata": {},
   "source": [
    "Spark Context :\n",
    "- Used to be the entry point for spark in the earlier versions say 1.x\n",
    "- Represents connection to spark cluster\n",
    "- Coordinates task execution across the cluster\n",
    "- Creates RDDs (Resilient Distributed Datasets)\n",
    "- Performs Transformations and defines actions.\n",
    "\n",
    "Spark Session :\n",
    "- The entry point for Spark since the version 2.0 that provides simple Interaction.\n",
    "- Combines the Functionalities like HiveContext, SparkContext, SQLContext and StreamingContext.\n",
    "- Supports multiple Programming Languages like : Scala, Java, R and Python\n",
    "- Extends the functionality of Spark Context.\n",
    "- Supports Advanced abstractions like Datasets and DataFrames\n",
    "- Provides Data Source APIs, Machine Learning Algorithms and streaming capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d09702",
   "metadata": {},
   "source": [
    "## Creating PySpark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8bbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a Spark Context Variable\n",
    "sc = SparkContext(appName=\"MySparkContext-Application\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dee3adc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://yateed3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MySparkContext-Application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=MySparkContext-Application>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8bb322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down the Spark Context\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69027b1",
   "metadata": {},
   "source": [
    "# Creating Spark Session with Manual Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2347fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Spark-Session-Manual-Config\") \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c57873e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://yateed3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark-Session-Manual-Config</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1c466dcef50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6062d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e262d53",
   "metadata": {},
   "source": [
    "# RDDs (Resilient Distributed Datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60273a29",
   "metadata": {},
   "source": [
    "- Backbone of data processing in Spark\n",
    "- Distributed, Fault-tolerant, parallelizable data structure\n",
    "- Efficiently processes large datasets across the cluster\n",
    "- RDDs are immutable, distributed, resilient, lazily evaluated, fault-tolerant\n",
    "- Fault Tolerant operations may contain : map, filter, reduce, collect, count, save, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa563fc",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "- Create new RDDs by applying computation/ Manipulation\n",
    "- Lazy Evaluation, Lineage Graph\n",
    "- Examples like map, filter, flatMap, reduceByKey, sortBy and join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3852fb4",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "- Return Results or perform actions on RDD, triggering execution\n",
    "- Eager evaluation, data movement/ computation\n",
    "- Examples like collect, count, first, take, save, foreach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d566426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDD-SparkSession\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e47ec9",
   "metadata": {},
   "source": [
    "## How to Create RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c689cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa7d2117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect method retrieves all elements from the RDD\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97ce1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from the List of Tuples\n",
    "employees = [(\"Ajay\", 8), (\"Raman\", 7), (\"Pratap\", 5), (\"Mohan\", 6), (\"Raman\", 3)]\n",
    "employees_rdd = spark.sparkContext.parallelize(employees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "826f1e0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the employee tuples :\n",
      "('Ajay', 8)\n",
      "('Raman', 7)\n",
      "('Pratap', 5)\n",
      "('Mohan', 6)\n",
      "('Raman', 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"All the employee tuples :\")\n",
    "\n",
    "## How to Create RDDs# Print the tuples in new line\n",
    "for i in employees_rdd.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b668b8f",
   "metadata": {},
   "source": [
    "## RDDs - Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "470fb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of items in the RDD : 5\n"
     ]
    }
   ],
   "source": [
    "# Count() action is used to count the items in the RDD.\n",
    "\n",
    "# Create a count variable to store the number of items in the rdd\n",
    "rdd_count = rdd.count()\n",
    "print(\"The total number of items in the RDD :\",rdd_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f102697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of items in the Employee RDD is :  5\n"
     ]
    }
   ],
   "source": [
    "employee_rdd_count = employees_rdd.count()\n",
    "print(\"The total number of items in the Employee RDD is : \", employee_rdd_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aad221c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first item in the RDD is :  ('Ajay', 8)\n"
     ]
    }
   ],
   "source": [
    "# first() action returns the first action from the RDD.\n",
    "first_item = employees_rdd.first()\n",
    "print(\"The first item in the RDD is : \", first_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91370fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The elements from the RDD are : \n",
      "('Ajay', 8)\n",
      "('Raman', 7)\n",
      "('Pratap', 5)\n"
     ]
    }
   ],
   "source": [
    "# take() action is used to retrieve the n number of elements from the RDD\n",
    "elements_needed = employees_rdd.take(3)\n",
    "print(\"The elements from the RDD are : \")\n",
    "for i in elements_needed :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9d4c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# foreach() action is used to print each element of the rdd\n",
    "employees_rdd.foreach(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5746a849",
   "metadata": {},
   "source": [
    "## RDDs - Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef78d1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Transformations, the data will be changed but only returns result when any action is performed\n",
    "\n",
    "# Map Transformations are done to convert the name to uppercase\n",
    "mapped_rdd = employees_rdd.map(lambda x: (x[0].upper(), x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89afab17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD in Upper case : [('AJAY', 8), ('RAMAN', 7), ('PRATAP', 5), ('MOHAN', 6), ('RAMAN', 3)]\n"
     ]
    }
   ],
   "source": [
    "map_result = mapped_rdd.collect()\n",
    "print(\"RDD in Upper case :\",map_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adbb3a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD with Experience of 7 years : [('Raman', 7)]\n"
     ]
    }
   ],
   "source": [
    "# filter Transformation : filter records based on any condition\n",
    "filtered_rdd = employees_rdd.filter(lambda x:x[1] == 7)\n",
    "\n",
    "filtered_result = filtered_rdd.collect()\n",
    "print(\"RDD with Experience of 7 years :\", filtered_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20a7e3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ajay', 8), ('Raman', 10), ('Pratap', 5), ('Mohan', 6)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ReduceBy Key : Calculate the total experience for each name\n",
    "reduced_rdd = employees_rdd.reduceByKey(lambda x,y: x + y)\n",
    "reduced_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f47e685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RDD in Ascending Order : [('Raman', 3), ('Pratap', 5), ('Mohan', 6), ('Raman', 7), ('Ajay', 8)]\n",
      "The RDD in Descending Order : [('Ajay', 8), ('Raman', 7), ('Mohan', 6), ('Pratap', 5), ('Raman', 3)]\n"
     ]
    }
   ],
   "source": [
    "# sortBy Transformation : This returns the data arranged in ascending or descending order\n",
    "sorted_rdd_asc = employees_rdd.sortBy(lambda x: x[1], ascending = True)\n",
    "print(\"The RDD in Ascending Order :\",sorted_rdd_asc.collect())\n",
    "\n",
    "sorted_rdd_desc = employees_rdd.sortBy(lambda x: x[1], ascending = False)\n",
    "print(\"The RDD in Descending Order :\",sorted_rdd_desc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d5ff0",
   "metadata": {},
   "source": [
    "# DataFrames - Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2f4a307",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.textFile(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40bbaec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = rdd.flatMap(lambda line: line.split(\" \"))\\\n",
    ".map(lambda word: (word, 1))\\\n",
    ".reduceByKey(lambda a, b: a+b)\\\n",
    ".sortBy(lambda x: x[1], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d9ac6618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 12),\n",
       " ('a', 7),\n",
       " ('of', 7),\n",
       " ('in', 5),\n",
       " ('distributed', 5),\n",
       " ('Spark', 4),\n",
       " ('is', 3),\n",
       " ('API', 3),\n",
       " ('as', 3),\n",
       " ('on', 3),\n",
       " ('Dataset', 3),\n",
       " ('RDD', 3),\n",
       " ('its', 2),\n",
       " ('data', 2),\n",
       " ('cluster', 2),\n",
       " ('that', 2),\n",
       " ('The', 2),\n",
       " ('was', 2),\n",
       " ('API.', 2),\n",
       " ('and', 2)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a44440aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "af5b2f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = df.selectExpr(\"explode(split(value, ' ')) as word\")\\\n",
    ".groupBy(\"word\").count().orderBy(\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b41d19b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result_df\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:1256\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[0;32m   1238\u001b[0m \n\u001b[0;32m   1239\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;124;03m    [Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m   1257\u001b[0m         sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\pyspark\\traceback_utils.py:75\u001b[0m, in \u001b[0;36mSCCallSiteSync.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msetCallSite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_site)\n\u001b[0;32m     76\u001b[0m     SCCallSiteSync\u001b[38;5;241m.\u001b[39m_spark_stack_depth \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_new_connection()\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 291\u001b[0m     connection\u001b[38;5;241m.\u001b[39mconnect_to_java_server()\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[1;32mC:\\anaconda\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[1;32m--> 438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mconnect((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_port))\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "result_df.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
