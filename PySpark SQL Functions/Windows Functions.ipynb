{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7c468f-6995-4894-af56-c8006f4700f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f25f073-0219-4ab2-bbdb-ef5a863a54a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e4dce7-367a-4bfa-bf32-83504314c9fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9274a44d-040c-4d00-b622-d9365244cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Window Functions\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9d9c018-984f-403a-bd59-cbb8fa490c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100), \\\n",
    "    (\"Ramesh\", \"Finance\", 4100) \\\n",
    "  )\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e4a02f-e5c4-41b2-85a4-1185e0238cde",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Window Ranking Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802684e-93ee-4854-af64-7857fbe85638",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80828607-2fe8-482c-900c-88bb86e8f81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         2|\n",
      "|       Robert|     Sales|  4100|         3|\n",
      "|         Saif|     Sales|  4100|         4|\n",
      "|      Michael|     Sales|  4600|         5|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.window import Window\n",
    "windowSpec = Window.partitionBy(\"Department\").orderBy(\"salary\")\n",
    "df.withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6629d91d-b551-44c4-bbf4-3ee8f8d944b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|          Jen|   Finance|  3900|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|        Maria|   Finance|  3000|         3|\n",
      "|         Jeff| Marketing|  3000|         1|\n",
      "|        Kumar| Marketing|  2000|         2|\n",
      "|      Michael|     Sales|  4600|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         3|\n",
      "|        James|     Sales|  3000|         4|\n",
      "|        James|     Sales|  3000|         5|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order them in descending\n",
    "from pyspark.sql.functions import row_number, desc\n",
    "from pyspark.sql.window import Window\n",
    "windowSpec = Window.partitionBy(\"Department\").orderBy(desc(\"salary\"))\n",
    "df.withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca067e8-3cfb-4c94-9407-c867d7171811",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Rank Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3194b7a-f8af-4d2a-9e87-5d78498b3abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"rank\", rank().over(windowSpec))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb719c59-f7c7-4aa7-864c-be49a5a8143e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dense Rank Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c148c1a7-d1a3-4cbc-bde8-581a90d2c535",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\", dense_rank().over(windowSpec))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950fd9c1-ec50-4b7f-8b4c-f7d5b8ffc934",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Percent_Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d47ac3f-33b4-48a4-b64b-a32145b5f489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\", percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cd3da2-820b-48a4-bdbe-6370ca1c697c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### ntile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75dfc829-b963-46d4-b6ce-253bd0c9662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|       Ramesh|   Finance|  4100|    3|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    2|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    3|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\", ntile(3).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0685786-0266-4bcb-8a8c-8796b77fedfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Window Analytic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04784d0a-eefb-4eb6-9870-21644fe6d522",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### cume_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "572e2e83-a76a-4838-8c02-234c1b13c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---------+\n",
      "|employee_name|department|salary|cume_dist|\n",
      "+-------------+----------+------+---------+\n",
      "|        Maria|   Finance|  3000|     0.25|\n",
      "|        Scott|   Finance|  3300|      0.5|\n",
      "|          Jen|   Finance|  3900|     0.75|\n",
      "|       Ramesh|   Finance|  4100|      1.0|\n",
      "|        Kumar| Marketing|  2000|      0.5|\n",
      "|         Jeff| Marketing|  3000|      1.0|\n",
      "|        James|     Sales|  3000|      0.4|\n",
      "|        James|     Sales|  3000|      0.4|\n",
      "|       Robert|     Sales|  4100|      0.8|\n",
      "|         Saif|     Sales|  4100|      0.8|\n",
      "|      Michael|     Sales|  4600|      1.0|\n",
      "+-------------+----------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist\n",
    "df.withColumn(\"cume_dist\", cume_dist().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590cd283-b150-40d3-9b7e-ea23980cb26b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lag function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18a90c2d-1b3b-4a5d-aeb4-1577b7f78530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|3000|\n",
      "|          Jen|   Finance|  3900|3300|\n",
      "|       Ramesh|   Finance|  4100|3900|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|2000|\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|3000|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|4100|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "df.withColumn(\"lag\", lag(\"salary\", 1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50408c13-64d8-4066-9d76-833b40495cf5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lead Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4cbae402-c6cd-422f-afe6-1a5b21b2750b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|4100|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|       Ramesh|   Finance|  4100|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead\n",
    "df.withColumn(\"lead\", lead(\"salary\", 2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c15279f-3781-412d-a238-fd341cc52017",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Window Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38eae817-ca57-4b7a-94df-694be0f575d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|   avg|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|   Finance|3575.0|14300|3000|4100|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "518a2b76-6f36-4dc7-b1f4-13b35cd1b065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+----+----+-----+\n",
      "|department|  sum|   avg| max| min|count|\n",
      "+----------+-----+------+----+----+-----+\n",
      "|   Finance|14300|3575.0|4100|3000|    4|\n",
      "| Marketing| 5000|2500.0|3000|2000|    2|\n",
      "|     Sales|18800|3760.0|4600|3000|    5|\n",
      "+----------+-----+------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, avg, max, min, count\n",
    "windowSpecAgg = Window.partitionBy(\"department\")\n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "    .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "    .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "    .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "    .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "    .withColumn(\"count\", count(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "    .where(col(\"row\")==1).select(\"department\",\"sum\",\"avg\",\"max\",\"min\",\"count\") \\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
