{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e016f8-556b-4ca4-9d2f-6d38ae72fc68",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## System Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63fd692e-8467-42d4-a1a2-2482905f10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0db62-d93e-464c-a92a-acb90f33ca60",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3d44ec-d287-4194-a0c5-f5e1d4fbbe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Date Function\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ae04c2-34bb-4693-9029-c3203a2cea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-04-05\"]]\n",
    "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef77a8-df6a-4b5a-8cbb-5078d51f72b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Date Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd15c9e-4b46-4782-bea2-42e03e775e7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### current_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118600e1-70db-4f54-918e-15365329574f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|currentDate|\n",
      "+-----------+\n",
      "| 2024-02-04|\n",
      "| 2024-02-04|\n",
      "| 2024-02-04|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# current_date returns the current date\n",
    "from pyspark.sql.functions import current_date\n",
    "df.select(current_date().alias(\"currentDate\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c185d5c-d4e4-4829-8ed3-bf85eb3a16de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### to_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae25c58-e3a6-49fc-bb35-7b5db07fb4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|     input|to_date|\n",
      "+----------+-------+\n",
      "|2020-02-01|   null|\n",
      "|2019-03-01|   null|\n",
      "|2021-04-05|   null|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_date converts the string type to date type\n",
    "from pyspark.sql.functions import to_date, col\n",
    "df1 = df.select(col(\"input\"),\n",
    "         to_date(col(\"input\"), \"dd-MM-yyyy\").alias(\"to_date\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2858d3a1-69d9-470c-a058-58a2706642a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: string (nullable = true)\n",
      " |-- to_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bd62715-8862-4cc1-be36-c189f5ef6c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- input: string (nullable = true)\n",
      " |-- new_input: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"new_input\", to_date(\"input\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0b77a0-dca3-408b-9f93-050aa8a85ce1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ce27429-1d84-44fa-b2c4-284a7e50d843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- input: string (nullable = true)\n",
      " |-- Day_Of_Week: integer (nullable = true)\n",
      "\n",
      "+----------+-----------+\n",
      "|     input|Day_Of_Week|\n",
      "+----------+-----------+\n",
      "|2020-02-01|          7|\n",
      "|2019-03-01|          6|\n",
      "|2021-04-05|          2|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "df2 = df.select(col(\"input\"), dayofweek(\"input\").alias(\"Day_Of_Week\"))\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaa5c71-9860-4081-b94d-aae9a2d08f87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dayofmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64856499-6a4e-451d-a9d1-53ab15cecb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|     input|Day_of_Month|\n",
      "+----------+------------+\n",
      "|2020-02-01|           1|\n",
      "|2019-03-01|           1|\n",
      "|2021-04-05|           5|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofmonth\n",
    "df.select(col(\"input\"), dayofmonth(\"input\").alias(\"Day_of_Month\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadf163-114c-4ab3-9d62-501f80b13b02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5281ab-e20c-498e-ba13-d7b8acae8177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+\n",
      "| id|     input|Day_Of_Year|\n",
      "+---+----------+-----------+\n",
      "|  1|2020-02-01|         32|\n",
      "|  2|2019-03-01|         60|\n",
      "|  3|2021-04-05|         95|\n",
      "+---+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofyear\n",
    "df.withColumn(\"Day_Of_Year\", dayofyear(\"input\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83119450-e815-42eb-8126-a0a01c166eeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### weekofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcadf960-cf28-463f-828c-b29cb6a356da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------+\n",
      "| id|     input|Week_Of_Year|\n",
      "+---+----------+------------+\n",
      "|  1|2020-02-01|           5|\n",
      "|  2|2019-03-01|           9|\n",
      "|  3|2021-04-05|          14|\n",
      "+---+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import weekofyear\n",
    "df.withColumn(\"Week_Of_Year\", weekofyear(\"input\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd537f40-61aa-430b-9bc7-f59d49de1a50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### year, month, quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "090bddf3-dc44-4d60-843b-53b4b8b3f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----+-----+-------+\n",
      "| id|     input|Year|Month|Quarter|\n",
      "+---+----------+----+-----+-------+\n",
      "|  1|2020-02-01|2020|    2|      1|\n",
      "|  2|2019-03-01|2019|    3|      1|\n",
      "|  3|2021-04-05|2021|    4|      2|\n",
      "+---+----------+----+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# quarter returns the quarter number (upto 3 months - 1st quarter, 4 to 6 months - 2nd quarter, ...)\n",
    "from pyspark.sql.functions import year, month, quarter\n",
    "df.withColumn(\"Year\", year(\"input\"))\\\n",
    "    .withColumn(\"Month\", month(\"input\"))\\\n",
    "    .withColumn(\"Quarter\", quarter(\"input\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc11691-fd47-465c-bc60-000f6065d1ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### last_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71c325e0-052c-4022-b3f8-10f8b459fb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|     input|   lastDay|\n",
      "+---+----------+----------+\n",
      "|  1|2020-02-01|2020-02-29|\n",
      "|  2|2019-03-01|2019-03-31|\n",
      "|  3|2021-04-05|2021-04-30|\n",
      "+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import last_day\n",
    "df.withColumn(\"lastDay\", last_day(\"input\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89d371a-e655-4411-acc6-84f104823476",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### next_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e93f7502-3641-42e1-8f15-dc7a7eb49b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|     input|   nextDay|\n",
      "+---+----------+----------+\n",
      "|  1|2020-02-01|2020-02-03|\n",
      "|  2|2019-03-01|2019-03-04|\n",
      "|  3|2021-04-05|2021-04-12|\n",
      "+---+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import next_day\n",
    "df.withColumn(\"nextDay\", next_day(\"input\", \"Monday\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea152c64-dd27-46b2-a4c1-2a94ea7f9f67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### add_months, date_add, date_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebba178e-ae59-427f-8f8a-5051c79cf517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+---------------+----------+----------+\n",
      "| id|     input| add_month|add_month_with-|  date_add|  date_sub|\n",
      "+---+----------+----------+---------------+----------+----------+\n",
      "|  1|2020-02-01|2020-05-01|     2019-11-01|2020-02-05|2020-01-30|\n",
      "|  2|2019-03-01|2019-06-01|     2018-12-01|2019-03-05|2019-02-27|\n",
      "|  3|2021-04-05|2021-07-05|     2021-01-05|2021-04-09|2021-04-03|\n",
      "+---+----------+----------+---------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import add_months, date_add, date_sub\n",
    "df.withColumn(\"add_month\", add_months(\"input\", 3))\\\n",
    "    .withColumn(\"add_month_with-\", add_months(\"input\", -3))\\\n",
    "    .withColumn(\"date_add\", date_add(\"input\", 4)) \\\n",
    "    .withColumn(\"date_sub\", date_sub(\"input\", 2)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f8b20-f218-41d1-8a3c-2856c407bb95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### datediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d05850e-b98e-4638-a975-4189d531deff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+\n",
      "|     input|date_difference|\n",
      "+----------+---------------+\n",
      "|2020-02-01|           1464|\n",
      "|2019-03-01|           1801|\n",
      "|2021-04-05|           1035|\n",
      "+----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "df.select(col(\"input\"),\\\n",
    "         datediff(current_date(), col(\"input\")).alias(\"date_difference\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74bc992-b0ea-4136-b5bd-5b3e5051f997",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### months_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae13ef0d-0d1c-4512-b89a-ac5f2dae09a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+\n",
      "| id|     input|    months_between|\n",
      "+---+----------+------------------+\n",
      "|  1|2020-02-01|48.096774193548384|\n",
      "|  2|2019-03-01|59.096774193548384|\n",
      "|  3|2021-04-05| 33.96774193548387|\n",
      "+---+----------+------------------+\n",
      "\n",
      "+---+----------+--------------+\n",
      "| id|     input|months_between|\n",
      "+---+----------+--------------+\n",
      "|  1|2020-02-01|   48.09677419|\n",
      "|  2|2019-03-01|   59.09677419|\n",
      "|  3|2021-04-05|   33.96774194|\n",
      "+---+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between\n",
    "df.withColumn(\"months_between\", months_between(current_date(), col(\"input\"), False)).show()\n",
    "df.withColumn(\"months_between\", months_between(current_date(), col(\"input\"), True)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51062ce3-5170-42ac-a7a1-5a25c280bc9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b458b661-ec7d-4d0f-a778-dd5247b7d3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------+--------------+------------+\n",
      "| id|     input|truncatedYear|truncatedMonth|truncatedDay|\n",
      "+---+----------+-------------+--------------+------------+\n",
      "|  1|2020-02-01|   2020-01-01|    2020-02-01|  2020-01-27|\n",
      "|  2|2019-03-01|   2019-01-01|    2019-03-01|  2019-02-25|\n",
      "|  3|2021-04-05|   2021-01-01|    2021-04-01|  2021-04-05|\n",
      "+---+----------+-------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import trunc\n",
    "df.withColumn(\"truncatedYear\", trunc(\"input\", \"Year\"))\\\n",
    "    .withColumn(\"truncatedMonth\", trunc(\"input\", \"Month\"))\\\n",
    "    .withColumn(\"truncatedDay\", trunc(\"input\", \"Week\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb5f882-bb2f-4c83-bda8-6df120a163e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### date_trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fd23d16-f70b-4838-920d-2b802b347e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+-------------------+-------------------+-------------------+\n",
      "| id|     input|      truncatedYear|     truncatedMonth|       truncatedDay|   truncatedQuarter|\n",
      "+---+----------+-------------------+-------------------+-------------------+-------------------+\n",
      "|  1|2020-02-01|2020-01-01 00:00:00|2020-02-01 00:00:00|2020-02-01 00:00:00|2020-01-01 00:00:00|\n",
      "|  2|2019-03-01|2019-01-01 00:00:00|2019-03-01 00:00:00|2019-03-01 00:00:00|2019-01-01 00:00:00|\n",
      "|  3|2021-04-05|2021-01-01 00:00:00|2021-04-01 00:00:00|2021-04-05 00:00:00|2021-04-01 00:00:00|\n",
      "+---+----------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_trunc\n",
    "df.withColumn(\"truncatedYear\", date_trunc(\"Year\", \"input\"))\\\n",
    "    .withColumn(\"truncatedMonth\", date_trunc(\"Month\", \"input\"))\\\n",
    "    .withColumn(\"truncatedDay\", date_trunc(\"Day\", \"input\"))\\\n",
    "    .withColumn(\"truncatedQuarter\", date_trunc(\"Quarter\", \"input\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "093687ed-e07d-4239-9a8b-7a72dc612c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-------------------+-------------------+-------------------+-------------------+\n",
      "| id|     input|      truncatedWeek|      truncatedHour|    truncatedMinute|    truncatedSecond|\n",
      "+---+----------+-------------------+-------------------+-------------------+-------------------+\n",
      "|  1|2020-02-01|2020-01-27 00:00:00|2020-02-01 00:00:00|2020-02-01 00:00:00|2020-02-01 00:00:00|\n",
      "|  2|2019-03-01|2019-02-25 00:00:00|2019-03-01 00:00:00|2019-03-01 00:00:00|2019-03-01 00:00:00|\n",
      "|  3|2021-04-05|2021-04-05 00:00:00|2021-04-05 00:00:00|2021-04-05 00:00:00|2021-04-05 00:00:00|\n",
      "+---+----------+-------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Remaining Most possible values : ‘second’, ‘minute’, ‘hour’, ‘week’\n",
    "from pyspark.sql.functions import date_trunc\n",
    "df.withColumn(\"truncatedWeek\", date_trunc(\"week\", \"input\"))\\\n",
    "    .withColumn(\"truncatedHour\", date_trunc(\"hour\", \"input\"))\\\n",
    "    .withColumn(\"truncatedMinute\", date_trunc(\"minute\", \"input\"))\\\n",
    "    .withColumn(\"truncatedSecond\", date_trunc(\"second\", \"input\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d907565-d8b8-47f5-b102-5bdb84b12dd5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "115b5b7d-5708-4bbe-839f-6f12333f6112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+\n",
      "|unix_timestamp|          timestamp|\n",
      "+--------------+-------------------+\n",
      "|    1612345678|2021-02-03 15:17:58|\n",
      "|    1623456789|2021-06-12 05:43:09|\n",
      "|    1634567890|2021-10-18 20:08:10|\n",
      "+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "# Sample DataFrame with Unix timestamps\n",
    "data = [(1612345678,),\n",
    "        (1623456789,),\n",
    "        (1634567890,)]\n",
    "columns = [\"unix_timestamp\"]\n",
    "df2 = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Using from_unixtime to convert Unix timestamps to timestamps\n",
    "df2 = df2.withColumn(\"timestamp\", from_unixtime(\"unix_timestamp\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b835b8-bec4-4592-b566-8915a03ba986",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e6b5894-4df5-4186-a366-a306662a5a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+\n",
      "| id|     input|timestamp|\n",
      "+---+----------+---------+\n",
      "|  1|2020-02-01|     null|\n",
      "|  2|2019-03-01|     null|\n",
      "|  3|2021-04-05|     null|\n",
      "+---+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "df.withColumn(\"timestamp\", unix_timestamp(\"input\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446ade9-467e-4a03-ab62-176f45b6a847",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Time Stamp Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c34db47c-e5cd-46b1-a0d2-d6e60e9a8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
    "df2=spark.createDataFrame(data,[\"id\",\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fca1ad46-05d3-4303-99e6-78911f431526",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
    "df3=spark.createDataFrame(data,[\"id\",\"input\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1fcd64-e6f1-4ad7-b59d-1d9787b5e54c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "994f92f9-eb72-4cd0-8ce1-51bdc0f1715c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-------------------------+\n",
      "|id |input                  |Current_Time             |\n",
      "+---+-----------------------+-------------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |2024-02-04 19:23:25.14265|\n",
      "|2  |03-01-2019 12 01 19 406|2024-02-04 19:23:25.14265|\n",
      "|3  |03-01-2021 12 01 19 406|2024-02-04 19:23:25.14265|\n",
      "+---+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "df2.withColumn(\"Current_Time\", current_timestamp()).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc7ab4-96da-4ee5-b32f-c10df1dc152d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### to_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21c660a6-59df-4223-885c-32be4d90a0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+-----------------------+\n",
      "|id |input                  |Corr_TS                |\n",
      "+---+-----------------------+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |2020-01-02 11:01:19.06 |\n",
      "|2  |03-01-2019 12 01 19 406|2019-01-03 12:01:19.406|\n",
      "|3  |03-01-2021 12 01 19 406|2021-01-03 12:01:19.406|\n",
      "+---+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df2.withColumn(\"Corr_TS\", to_timestamp(\"input\", \"dd-MM-yyyy HH mm ss SSS\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3051e9b-5e1d-4772-899e-38a48a74da4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc7be8cc-0534-4a9a-a1d2-1b9e7ff01540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+----+\n",
      "| id|               input|hour|\n",
      "+---+--------------------+----+\n",
      "|  1|2020-02-01 11:01:...|  11|\n",
      "|  2|2019-03-01 12:01:...|  12|\n",
      "|  3|2021-03-01 12:01:...|  12|\n",
      "+---+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour\n",
    "df3.withColumn(\"hour\", hour(\"input\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bf6e04-a4a4-4644-981d-f3954f4ac3d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "607b9115-1633-4b83-9025-6e1188f3f8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+---+\n",
      "|id |input                  |min|\n",
      "+---+-----------------------+---+\n",
      "|1  |2020-02-01 11:01:19.06 |1  |\n",
      "|2  |2019-03-01 12:01:19.406|1  |\n",
      "|3  |2021-03-01 12:01:19.406|1  |\n",
      "+---+-----------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import minute\n",
    "df3.withColumn(\"min\", minute(\"input\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1e525-fd23-4af9-9e1a-08e88be4e439",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65dbdfeb-f95d-4008-a1ac-efbe16f6e329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+---+\n",
      "|id |input                  |sec|\n",
      "+---+-----------------------+---+\n",
      "|1  |2020-02-01 11:01:19.06 |19 |\n",
      "|2  |2019-03-01 12:01:19.406|19 |\n",
      "|3  |2021-03-01 12:01:19.406|19 |\n",
      "+---+-----------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import second\n",
    "df3.withColumn(\"sec\", second(\"input\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22c3cac-3001-41ad-ba8e-af6a24e70fe2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Date Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24e9d339-e2cc-459a-92e5-3b91a236d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"2022-01-15 08:30:45\",),\n",
    "        (\"2022-02-20 12:15:30\",),\n",
    "        (\"2022-03-25 18:45:15\",)]\n",
    "columns = [\"timestamp\"]\n",
    "df4 = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Convert the string timestamp to a timestamp type\n",
    "df4 = df4.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8860a953-38cf-444e-bb86-18c014ea8641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|          timestamp|new_format|\n",
      "+-------------------+----------+\n",
      "|2022-01-15 08:30:45|15-01-2022|\n",
      "|2022-02-20 12:15:30|20-02-2022|\n",
      "|2022-03-25 18:45:15|25-03-2022|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "df4.withColumn(\"new_format\", date_format(\"timestamp\", \"dd-MM-yyyy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c8f8a4a-d7bf-4a5c-a05d-5238416653e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+\n",
      "|          timestamp|year|\n",
      "+-------------------+----+\n",
      "|2022-01-15 08:30:45|2022|\n",
      "|2022-02-20 12:15:30|2022|\n",
      "|2022-03-25 18:45:15|2022|\n",
      "+-------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"year\", date_format(\"timestamp\", \"yyyy\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb4e7e28-5dad-4ec5-8c4b-82770e83cae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|          timestamp|month|\n",
      "+-------------------+-----+\n",
      "|2022-01-15 08:30:45|   01|\n",
      "|2022-02-20 12:15:30|   02|\n",
      "|2022-03-25 18:45:15|   03|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"month\", date_format(\"timestamp\", \"MM\")) \\\n",
    "       .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc70f7c8-35ab-43a1-af28-292dd2101576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|          timestamp|short_month|\n",
      "+-------------------+-----------+\n",
      "|2022-01-15 08:30:45|        Jan|\n",
      "|2022-02-20 12:15:30|        Feb|\n",
      "|2022-03-25 18:45:15|        Mar|\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"short_month\", date_format(\"timestamp\", \"MMM\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40bce883-732a-41d8-bcca-6223e950da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|          timestamp|full_month|\n",
      "+-------------------+----------+\n",
      "|2022-01-15 08:30:45|   January|\n",
      "|2022-02-20 12:15:30|  February|\n",
      "|2022-03-25 18:45:15|     March|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"full_month\", date_format(\"timestamp\", \"MMMM\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbde90f5-12b1-4123-9251-294b452b3927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|          timestamp|am_pm|\n",
      "+-------------------+-----+\n",
      "|2022-01-15 08:30:45|   AM|\n",
      "|2022-02-20 12:15:30|   PM|\n",
      "|2022-03-25 18:45:15|   PM|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"am_pm\", date_format(\"timestamp\", \"a\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc8a1c-dba8-4ddc-a5b3-e3fb3440159a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Timestamp Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58a036e4-4c61-410c-abbe-b4d6ffd4a89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+\n",
      "|timestamp          |am_pm                 |\n",
      "+-------------------+----------------------+\n",
      "|2022-01-15 08:30:45|2022-01-15 08:30:45 AM|\n",
      "|2022-02-20 12:15:30|2022-02-20 12:15:30 PM|\n",
      "|2022-03-25 18:45:15|2022-03-25 06:45:15 PM|\n",
      "+-------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df4.withColumn(\"timestamp\", col(\"timestamp\").cast(\"timestamp\"))\n",
    "\n",
    "# a in the format explains it if it is AM or PM by converting the 24h into 12h\n",
    "df4 = df4.withColumn(\"am_pm\", date_format(\"timestamp\", \"yyyy-MM-dd hh:mm:ss a\"))\n",
    "\n",
    "df4.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04d74fba-5637-4d7e-9fa2-d95501a3ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-------------------+\n",
      "|          timestamp|               am_pm|     formatted_date|\n",
      "+-------------------+--------------------+-------------------+\n",
      "|2022-01-15 08:30:45|2022-01-15 08:30:...|15-01-2022 08:30:45|\n",
      "|2022-02-20 12:15:30|2022-02-20 12:15:...|20-02-2022 12:15:30|\n",
      "|2022-03-25 18:45:15|2022-03-25 06:45:...|25-03-2022 18:45:15|\n",
      "+-------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"formatted_date\", date_format(\"timestamp\", \"dd-MM-yyyy HH:mm:ss\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8132ae2-0630-4654-a13a-dfe84f7ffc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n",
      "|          timestamp|               am_pm|        abbrev_month|\n",
      "+-------------------+--------------------+--------------------+\n",
      "|2022-01-15 08:30:45|2022-01-15 08:30:...|15/Jan/2022 08:30:45|\n",
      "|2022-02-20 12:15:30|2022-02-20 12:15:...|20/Feb/2022 12:15:30|\n",
      "|2022-03-25 18:45:15|2022-03-25 06:45:...|25/Mar/2022 18:45:15|\n",
      "+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"abbrev_month\", date_format(\"timestamp\", \"dd/MMM/yyyy HH:mm:ss\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23c6f5e7-48cb-44c7-9e92-de413dc9a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+\n",
      "|          timestamp|               am_pm|          month_year|\n",
      "+-------------------+--------------------+--------------------+\n",
      "|2022-01-15 08:30:45|2022-01-15 08:30:...|Jan-2022 08:30:45 AM|\n",
      "|2022-02-20 12:15:30|2022-02-20 12:15:...|Feb-2022 12:15:30 PM|\n",
      "|2022-03-25 18:45:15|2022-03-25 06:45:...|Mar-2022 18:45:15 PM|\n",
      "+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"month_year\", date_format(\"timestamp\", \"MMM-yyyy HH:mm:ss a\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5730706-3d5d-44ab-9e34-3de407f581f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+\n",
      "|          timestamp|               am_pm|day_of_week|\n",
      "+-------------------+--------------------+-----------+\n",
      "|2022-01-15 08:30:45|2022-01-15 08:30:...|   Saturday|\n",
      "|2022-02-20 12:15:30|2022-02-20 12:15:...|     Sunday|\n",
      "|2022-03-25 18:45:15|2022-03-25 06:45:...|     Friday|\n",
      "+-------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"day_of_week\", date_format(\"timestamp\", \"EEEE\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "442bfe03-a6f9-4bfc-8183-f09316d7f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+-----------+\n",
      "|          timestamp|               am_pm|day_of_week|\n",
      "+-------------------+--------------------+-----------+\n",
      "|2022-01-15 08:30:45|2022-01-15 08:30:...|        Sat|\n",
      "|2022-02-20 12:15:30|2022-02-20 12:15:...|        Sun|\n",
      "|2022-03-25 18:45:15|2022-03-25 06:45:...|        Fri|\n",
      "+-------------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.withColumn(\"day_of_week\", date_format(\"timestamp\", \"E\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
