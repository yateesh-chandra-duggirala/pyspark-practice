{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31809bdd-e28d-4c96-81cc-7d3dfca7dd61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457320d0-ad7f-441d-b50d-a968adc3ca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f578e751-e979-42ec-87ad-70cfdb55c8f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7b4f94-0fbe-4273-b2e1-84b925fb0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"Aggregate Functions\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f98c548-1777-4442-ac23-55252a291c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  ]\n",
    "schema = [\"employee_name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b1314-7049-4e17-92c6-e2a85b0f37ee",
   "metadata": {},
   "source": [
    "## Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eeeaa7-db93-49f8-8d6d-6d1ff1f74e30",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### approx_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc7549e-3a47-45e0-9623-0097deb4b675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approx_count_distinct : 6\n"
     ]
    }
   ],
   "source": [
    "# This function returns the count of distinct values from a column\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "print(\"approx_count_distinct : \" + \\\n",
    "     str(df.select(approx_count_distinct(\"salary\")) \\\n",
    "         .collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9753441-b304-4c98-a9b8-a6d5ed7d64cd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95411bc-4d3d-4b62-b376-51380dc3ffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg of column : 3400.0\n"
     ]
    }
   ],
   "source": [
    "# This function returns the average of the values from the given column\n",
    "from pyspark.sql.functions import avg\n",
    "print(\"avg of column : \" + \\\n",
    "     str(df.select(avg(\"salary\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1ee97-445a-45c1-be0c-38f8f9ce8dfe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b10c3ec8-378a-4716-a5b1-181d5e824e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected list of values : ['James', 'Michael', 'Robert', 'Maria', 'James', 'Scott', 'Jen', 'Jeff', 'Kumar', 'Saif']\n"
     ]
    }
   ],
   "source": [
    "# This function returns the list of the non distinct values collected from the column \n",
    "from pyspark.sql.functions import collect_list\n",
    "print(\"collected list of values : \" + \\\n",
    "     str(df.select(collect_list(\"employee_name\")).collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80812bd7-ef9a-4cfe-b802-3853b2f9dc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------+\n",
      "|names                                                                |\n",
      "+---------------------------------------------------------------------+\n",
      "|[James, Michael, Robert, Maria, James, Scott, Jen, Jeff, Kumar, Saif]|\n",
      "+---------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(collect_list(\"employee_name\").alias(\"names\")).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26b9d1-9e4b-4c2c-8b62-01590df17523",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### collect_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abebb97b-be18-4aec-89bf-e18b94eec65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect set : [4600, 3000, 3900, 4100, 3300, 2000]\n"
     ]
    }
   ],
   "source": [
    "# This function returns the list of the distinct values collected from the column \n",
    "from pyspark.sql.functions import collect_set\n",
    "print(\"collect set : \" + \\\n",
    "     str(df.select(collect_set(\"salary\")) \\\n",
    "        .collect()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b351cb-e393-4193-bb80-b8ba099d9d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "| collect_set(salary)|\n",
      "+--------------------+\n",
      "|[4600, 3000, 3900...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list = df.select(collect_set(\"salary\"))\n",
    "list.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8a8a8-689a-48f6-97dd-b5bd242eaf5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### countDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71f62e66-f5d3-46e7-a4b3-c8cf609e131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|count(DISTINCT department, salary)|\n",
      "+----------------------------------+\n",
      "|                                 8|\n",
      "+----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9efb3a1-9b70-4062-8769-0e3ca4640a5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1445e4a-d726-454c-9164-d2c1b44eeaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(salary)|\n",
      "+-------------+\n",
      "|            9|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df2 = df.select(count(\"salary\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162bd0c8-103a-44c1-baa0-262d73820734",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd0e635b-5dc3-46ac-8247-274b669ff9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|first(salary)|\n",
      "+-------------+\n",
      "|         3000|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first\n",
    "df.select(first(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940e753-a7c5-484f-88d9-b22e774742db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### last()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ab31d42-cc5b-4daf-a3dd-29be15d5ae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|last(salary)|\n",
      "+------------+\n",
      "|        4100|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import last\n",
    "df.select(last(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e5fa96-20d7-473c-96af-7f4598ee7abc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50c1679c-eb20-4154-b917-5e6929924d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(salary)|\n",
      "+-----------+\n",
      "|       4600|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "df.select(max(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79c132-e21e-4577-acf0-b75398f3bba3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85f8abcd-0c45-422a-bebe-71681bbfcf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(salary)|\n",
      "+-----------+\n",
      "|       2000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min\n",
    "df.select(min(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db284c8b-09a8-4b1b-8d9a-95926f628413",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fee9056-e588-4cd4-b84b-76569a6e7c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(salary)|\n",
      "+-----------+\n",
      "|     3400.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1478e5-13ef-46e2-accb-213f375ed6ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bd8fadb-f928-412e-9dc8-0abd1af1f73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|   kurtosis(salary)|\n",
      "+-------------------+\n",
      "|-0.6467803030303032|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import kurtosis\n",
    "df.select(kurtosis(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5496bf-361a-455f-b197-ebf9e214a306",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8c3b19e-3d96-445f-b1a8-f95179bf03da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|    skewness(salary)|\n",
      "+--------------------+\n",
      "|-0.12041791181069571|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness\n",
    "df.select(skewness(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1774d-d60f-407b-97e5-410504ee4c65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d692247-0b26-47b6-97ab-bd78598e8886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|stddev_samp(salary)|\n",
      "+-------------------+\n",
      "|  765.9416862050705|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev\n",
    "df.select(stddev(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe65d759-e0bd-458a-8468-07ed8a9e52e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### stddev_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cd84dde-0a20-48ac-b6d2-a645f59497da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|stddev_samp(salary)|\n",
      "+-------------------+\n",
      "|  765.9416862050705|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev_samp\n",
    "df.select(stddev_samp(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8c403-2e54-4d4c-bb65-8e91618f9022",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### stddev_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76724775-1ea9-48ae-a268-2b40f1b7f758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|stddev_pop(salary)|\n",
      "+------------------+\n",
      "|  726.636084983398|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev_pop\n",
    "df.select(stddev_pop(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65afaa9-1b1d-4ef4-8078-7c0b69a8c14e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "708dc2af-8f2b-4717-8426-7b1164a3d43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(salary)|\n",
      "+-----------+\n",
      "|      34000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597dd52-cd3b-41cd-9248-d46bb55577f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "676cf304-5d16-46fd-9dbd-7d33143f408e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(DISTINCT salary)|\n",
      "+--------------------+\n",
      "|               20900|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum_distinct\n",
    "df.select(sum_distinct(\"salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015c80bd-cf70-4035-9197-b28adef42068",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f6ffba39-464a-4850-b412-81abaec775a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "| var_samp(salary)|\n",
      "+-----------------+\n",
      "|586666.6666666666|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import variance\n",
    "df.select(variance(\"salary\")).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
